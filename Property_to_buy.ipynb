{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 1 in Assessment 3\n",
    "#### Student Name: Sai Ram D Gitte\n",
    "#### Student ID: 31009751\n",
    "\n",
    "Date: 21/11/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.4 and Jupyter notebook\n",
    "\n",
    "\n",
    "## Task 1 - Data Integration\n",
    "\n",
    "#### Introduction:\n",
    "When someone is looking out to rent/buy a property, he/she looks at different aspects in and around the property to decide if they would want to live there. Collecting all the possible data for a property streamlines the process of narrowing down a property. Few of the aspects one would look into is if the property is located around basic necessities and amenities like:\n",
    "\n",
    "1. Supermarkets\n",
    "2. Shopping centers\n",
    "3. Hospitals\n",
    "4. Train stations\n",
    "\n",
    "Now, to extend the information provided for each property, the final result would also include details on which are the nearest supermarkets, shopping centers, hospitals and train stations, whats the average time taken to reach the City (i.e., Flinders Station) through train from the station that is closely situated to each of the properties. If there are direct trains \n",
    "\n",
    "Task 1 focuses on integarting data from different sources such as pdfs, txt, xml, json, xlsx, html and shp files into once csv file. Along with this, there are a few logical computations to be done in order to arrive at the desired columns and their values.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pdfminer \n",
    "!pip install pdfminer.six\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pdfminer.high_level import extract_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supermarket\n",
    "* To read the pdf I have used the 'pdfminer.six' package. In that, the module named 'extract_text' helps us to read the pdf file into a variable. \n",
    "* The read text is then run inside a loop, splitting the entire content on a newline '\\n' and the appending it to a list called 'data'. \n",
    "* A try and except block is used to check if the data that is split has a number as its first elemnet, if thats true, then append it to the list else do not do anything and go back to the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read supermarket pdf file\n",
    "text = extract_text('supermarkets.pdf') # Reading the pdf data into a variable\n",
    "\n",
    "data = [] #emply list \n",
    "for i in text.split('\\n'): # Looping through the split data\n",
    "    if i != '':\n",
    "        try:\n",
    "            if int(i[0]) or i[0] == '0':\n",
    "                data.append(i.split(' ')) #updating the empty list \n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting read pdf into a dataframe\n",
    "\n",
    "df_supermarket = pd.DataFrame(data, columns=['dummy', 'id','lat','lng', 'type']) \n",
    "\n",
    "df_supermarket.drop(['dummy'], axis = 1, inplace = True) # Dropping the 'dummy' column from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supermarket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hospital id\n",
    "* To read the data into pandas dataframe, I have used the read_excel function within pandas\n",
    "* This will read the data into a dataframe format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installing optional dependency xlrd\n",
    "# !pip install xlrd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hospital = pd.read_excel('hospitals.xlsx', index_col=0)\n",
    "df_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shopping Center\n",
    "* To read the 'html' file into a dataframe, pandas.read_html() is used. \n",
    "* pd.read_html returns the read dataframe into list of dataframes. \n",
    "* Here there is only one element in the list of dataframe and hence 'df_shopping[0]' is used to retrieve the dataframe\n",
    "* Drop the unwanted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_shopping = pd.read_html('shopingcenters.html') # Reading the html file into a dataframe\n",
    "df_shopping = df_shopping[0] # \n",
    "df_shopping.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "df_shopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real estate\n",
    "## There are two types of file here: 'json' and 'xml':\n",
    "        To read the json file:\n",
    "            1. Use the pandas.read_json() function which will directly read the json contents to a dataframe\n",
    "        \n",
    "        To read the xml file:\n",
    "        \n",
    "            1. Read it using the 'open()' function in read mode and passing the encoding as 'utf-8'\n",
    "\n",
    "                - this returns a single string\n",
    "\n",
    "            2. Eliminate the first and last character of the above string (unwanted characters)\n",
    "            3. Create a list of list which contains all the column values in it separately as different lists\n",
    "            4. Converting this lists of list into a dataframe with 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = pd.read_json(\"real_state.json\") # Reading the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"real_state.xml\", 'r', encoding = 'utf8') \n",
    "xml_str = file.readlines()[0][2:-1] # Reading the xml file into a string\n",
    "xml_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "root = ET.fromstring(xml_str) # Root has the data in a dictionary format\n",
    "\n",
    "main_list = [] # Empty list \n",
    "for i in range(len(root)): # Looping through the dictionary\n",
    "    sub_list = []\n",
    "    for j in range(len(root[i])):\n",
    "        sub_list.append(root[i][j].text) # Extract the values from the dictionary and update the list\n",
    "    main_list.append(sub_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names\n",
    "for i in root:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of column names\n",
    "colnames = ['property_id', 'lat', 'lng', 'addr_street', 'price', 'property_type', 'year', 'bedrooms', 'bathrooms', 'parking_space']\n",
    "\n",
    "# Create a pandas dataframe\n",
    "df_xml = pd.DataFrame(columns = colnames)\n",
    "\n",
    "# Assign the each list in the lists of list to the dataframe columns \n",
    "for i in range(len(root)):\n",
    "    df_xml[colnames[i]] = main_list[i]\n",
    "df_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTFS\n",
    "* Reading just the required files from GTFS to start working on nearest train station ids, travel min to CBD and Transfer flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Trips.txt file\n",
    "df_gtfs_trips = pd.read_csv('trips.txt', sep=\",\", header=None)\n",
    "\n",
    "# Defining the column names\n",
    "df_gtfs_trips.columns = [\"route_id\", \"service_id\", \"trip_id\", \"shape_id\",\"trip_headsign\", \"direction_id\"]\n",
    "\n",
    "# Except the the first row, retain everything\n",
    "df_gtfs_trips = df_gtfs_trips.iloc[1:] \n",
    "df_gtfs_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Stops.txt file\n",
    "df_gtfs_stops = pd.read_csv('stops.txt', sep=\",\", header=None)\n",
    "\n",
    "# Defining the column names\n",
    "df_gtfs_stops.columns = [\"stop_id\", \"stop_name\", \"stop_short_name\", \"stop_lat\",\"stop_lon\"]\n",
    "\n",
    "# Except the the first row, retain everything\n",
    "df_gtfs_stops = df_gtfs_stops.iloc[1:]\n",
    "df_gtfs_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Stop_times.txt file\n",
    "df_gtfs_stop_times = pd.read_csv('stop_times.txt', sep=\",\", header=None)\n",
    "\n",
    "# Defining the column names\n",
    "df_gtfs_stop_times.columns = [\"trip_id\", \"arrival_time\", \"departure_time\", \"stop_id\",\"stop_sequence\",\"stop_headsign\",\"pickup_type\",\"drop_off_type\",\"shape_dist_traveled\"]\n",
    "\n",
    "# Except the the first row, retain everything\n",
    "df_gtfs_stop_times = df_gtfs_stop_times.iloc[1:]\n",
    "df_gtfs_stop_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the departure_time column\n",
    "import re\n",
    "\n",
    "df_gtfs_stop_times_dup = df_gtfs_stop_times.copy()\n",
    "\n",
    "for item, row in df_gtfs_stop_times_dup.iterrows():\n",
    "    if re.match(r'^24', row['departure_time'].split(':')[0]):\n",
    "        df_gtfs_stop_times.loc[item, 'departure_time'] = re.findall(r'24', df_gtfs_stop_times_dup.loc[item,'departure_time'])[0].replace(re.findall(r'24', df_gtfs_stop_times_dup.loc[item,'departure_time'])[0], '00:00:00')\n",
    "        \n",
    "    if re.match(r'^25', row['departure_time'].split(':')[0]):\n",
    "        df_gtfs_stop_times.loc[item, 'departure_time'] = re.findall(r'25', df_gtfs_stop_times_dup.loc[item,'departure_time'])[0].replace(re.findall(r'25', df_gtfs_stop_times_dup.loc[item,'departure_time'])[0], '00:00:00')\n",
    "        \n",
    "    if re.match(r'^26', row['departure_time'].split(':')[0]):\n",
    "        df_gtfs_stop_times.loc[item, 'departure_time'] = re.findall(r'26', df_gtfs_stop_times_dup.loc[item,'departure_time'])[0].replace(re.findall(r'26', df_gtfs_stop_times_dup.loc[item,'departure_time'])[0], '00:00:00')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final dataframe\n",
    "* Combine json and xml converted dataframe into one dataframe\n",
    "* Drop any duplicate rows that are present after combing the dataframes\n",
    "    - 7 rows were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final df\n",
    "df_final = pd.concat([df_real, df_xml])\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop_duplicates()\n",
    "df_final = df_final.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haversine function\n",
    "- Defining a function to calculate the haversine distance between two coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distance between customer loc to the nearest warehouse\n",
    "from math import *\n",
    "\n",
    "def haversine_fun(latitude1, longitude1, latitude2, longitude2):\n",
    "    # Given that the radius of the earth should be taken 6378 km \n",
    "    earth_radius = 6378 \n",
    "    \n",
    "    # Take the difference of the two latitudes and convert them to radians\n",
    "    lat_diff = radians(latitude2 - latitude1) \n",
    "    \n",
    "    # Take the difference of the two Longitudes and convert them to radians\n",
    "    lon_diff = radians(longitude2 - longitude1) \n",
    "    \n",
    "    # Convert the first latitude to radians\n",
    "    latitude1 = radians(latitude1) \n",
    "    \n",
    "    # Convert the second latitude to radians\n",
    "    latitude2 = radians(latitude2) \n",
    "\n",
    "    x = sin(lat_diff/2)**2 + cos(latitude1)*cos(latitude2)*sin(lon_diff/2)**2 \n",
    "    y = 2*asin(sqrt(x))\n",
    "\n",
    "    return earth_radius * y # Returns the distance between two coordinates \n",
    "\n",
    "# Refer: From Assignment-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance shopping center\n",
    "- Converting the latitude and longitude values to float datatype\n",
    "- Calculate distance of each property location with each of the shopping centers\n",
    "- Then calculate which is the minimum distance and its indices\n",
    "- Loop through df_shopping dataframe and use the above mentioned indices to fetch the 'sc_id' of the shopping center\n",
    "- Update the final dataframe with the new values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the latitude and longitude values to float from string\n",
    "df_final['lat'] = df_final.lat.astype(float)\n",
    "df_final['lng'] = df_final.lng.astype(float)\n",
    "\n",
    "df_shopping['lat'] = df_shopping.lat.astype(float)\n",
    "df_shopping['lng'] = df_shopping.lng.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating all the distances from the shopping centre to the property \n",
    "main_dist_list = []\n",
    "for item, row in df_final.iterrows(): # Looping through the final dataframe\n",
    "    dist_list = []\n",
    "    for item1, row1 in df_shopping.iterrows():\n",
    "        \n",
    "        # Calculating the distance between 2 coordinates\n",
    "        dist_list.append(haversine_fun(\n",
    "            row['lat'],row['lng'], \n",
    "            row1['lat'], row1['lng']))\n",
    "        # Update the main list\n",
    "    main_dist_list.append(dist_list)\n",
    "main_dist_list # List of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the minumum distance from property to shopping center\n",
    "new = [] \n",
    "\n",
    "# List contains the minimum distance for all the rows\n",
    "min_dist = [] \n",
    "for i in main_dist_list:\n",
    "    new.append(i.index(min(i)))\n",
    "    \n",
    "    # Rounding off the distance to 4 decimals\n",
    "    min_dist.append(round(min(i),4)) \n",
    "column_name = np.array(new) \n",
    "\n",
    "# Creating a new column which will contain the \n",
    "# corresponding indices of shopping_centers names\n",
    "df_final['indexo'] = column_name \n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closest shopping center\n",
    "shopping_center_id = []\n",
    "\n",
    "# Looping through the final df and shopping df to retrieve the shopping center id\n",
    "for item, row in df_final.iterrows():\n",
    "    \n",
    "    shopping_center_id.append(df_shopping.loc[row['indexo'], 'sc_id'])\n",
    "\n",
    "# Update the final df with the shopping center id\n",
    "\n",
    "df_final['Shopping_center_id'] = shopping_center_id\n",
    "\n",
    "# Update the final df with the minimum distance to shopping center\n",
    "df_final['Distance_to_sc'] = min_dist\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(['index', 'indexo'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station ID\n",
    "- Converting the latitude and longitude values to float datatype\n",
    "- Calculate distance of each property location with each of the train centers\n",
    "- Then calculate which is the minimum distance and its indices\n",
    "- Loop through df_gtfs_stops dataframe and use the above mentioned indices to fetch the 'Train_station_id'\n",
    "- Update the final dataframe with the new values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the latitude and longitude \n",
    "df_gtfs_stops['stop_lat'] = df_gtfs_stops.stop_lat.astype(float)\n",
    "\n",
    "df_gtfs_stops['stop_lon'] = df_gtfs_stops.stop_lon.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of lists of distances\n",
    "main_dist_list = []\n",
    "for item, row in df_final.iterrows():\n",
    "    dist_list = []\n",
    "    # Loop through the stops file\n",
    "    for item1, row1 in df_gtfs_stops.iterrows(): \n",
    "        \n",
    "        # Calculate the distance between \n",
    "        # train_station and property\n",
    "        dist_list.append(haversine_fun(\n",
    "            row['lat'],row['lng'], \n",
    "            row1['stop_lat'], row1['stop_lon']))\n",
    "    main_dist_list.append(dist_list)\n",
    "main_dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = [] \n",
    "\n",
    "# List contains the minimum distance for all the rows\n",
    "min_dist = [] \n",
    "for i in main_dist_list:\n",
    "    new.append(i.index(min(i)))\n",
    "    \n",
    "    # Rounding off the distance to 4 decimals\n",
    "    min_dist.append(round(min(i),4)) \n",
    "column_name = np.array(new) \n",
    "\n",
    "# Creating a new column which will \n",
    "# contain the corresponding indices of station_id\n",
    "df_final['dist_index'] = column_name \n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closest station\n",
    "train_station_id = []\n",
    "\n",
    "# Loop through the final dataframe\n",
    "for item, row in df_final.iterrows():\n",
    "    \n",
    "    # Update the list with the corresponding station ids\n",
    "    train_station_id.append(df_gtfs_stops.loc[row['dist_index'], 'stop_id'])\n",
    "\n",
    "# Updating the train station id in the \n",
    "# final dataframe\n",
    "df_final['Train_station_id'] = train_station_id\n",
    "\n",
    "# Updating the train station dist in the \n",
    "# final dataframe\n",
    "df_final['Distance_to_train_station'] = min_dist\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(['dist_index'], axis = 1)\n",
    "df_final "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance to hospital\n",
    "- Converting the latitude and longitude values to float datatype\n",
    "- Calculate distance of each property location with each of the hospitals\n",
    "- Then calculate which is the minimum distance and its indices\n",
    "- Loop through df_hospitals dataframe and use the above mentioned indices to fetch the 'hospital_id'\n",
    "- Update the final dataframe with the new values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the string latitude and longitude values\n",
    "# to float datatype\n",
    "\n",
    "df_hospital['lat'] = df_hospital.lat.astype(float)\n",
    "df_hospital['lng'] = df_hospital.lng.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lists of distances\n",
    "main_dist_list = []\n",
    "\n",
    "# Loop through the final dataframe to calculate the \n",
    "# distances from property to hospitals\n",
    "for item, row in df_final.iterrows():\n",
    "    dist_list = []\n",
    "    for item1, row1 in df_hospital.iterrows():\n",
    "        dist_list.append(haversine_fun(\n",
    "            row['lat'],row['lng'], \n",
    "            row1['lat'], row1['lng']))\n",
    "    main_dist_list.append(dist_list)\n",
    "main_dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the minimum distance from nearest hospital\n",
    "# to the property\n",
    "\n",
    "new = [] \n",
    "\n",
    "# List contains the minimum distance for all the rows\n",
    "min_dist = [] \n",
    "for i in main_dist_list:\n",
    "    new.append(i.index(min(i)))\n",
    "    \n",
    "    # Rounding off the distance to 4 decimals\n",
    "    min_dist.append(round(min(i),4)) \n",
    "column_name = np.array(new) \n",
    "\n",
    "# Creating a new column which will contain the \n",
    "# corresponding indices of hospital ids\n",
    "df_final['hospital_index'] = column_name \n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closest hospital\n",
    "hospital_id = []\n",
    "\n",
    "# Looping through the final dataframe \n",
    "# and updating the corresponding hospital ids\n",
    "for item, row in df_final.iterrows():\n",
    "    \n",
    "    hospital_id.append(df_hospital.loc[row['hospital_index'], 'id'])\n",
    "\n",
    "# Updating the final dataframe\n",
    "df_final['Hospital_id'] = hospital_id\n",
    "df_final['Distance_to_hospital'] = min_dist\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(['hospital_index'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supermarket\n",
    "- Converting the latitude and longitude values to float datatype\n",
    "- Calculate distance of each property location with each of the supermarkets\n",
    "- Then calculate which is the minimum distance and its indices\n",
    "- Loop through df_supermarket dataframe and use the above mentioned indices to fetch the 'Supermarket_id'\n",
    "- Update the final dataframe with the new values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the string latitude and longitude\n",
    "# to float datatype\n",
    "df_supermarket['lat'] = df_supermarket.lat.astype(float)\n",
    "df_supermarket['lng'] = df_supermarket.lng.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of lists of distances\n",
    "main_dist_list = []\n",
    "\n",
    "# Looping through final dataframe to calculate \n",
    "# distances from each property to each of the \n",
    "# Supermarkets\n",
    "for item, row in df_final.iterrows():\n",
    "    dist_list = []\n",
    "    for item1, row1 in df_supermarket.iterrows():\n",
    "        dist_list.append(haversine_fun(\n",
    "            row['lat'],row['lng'], \n",
    "            row1['lat'], row1['lng']))\n",
    "        \n",
    "    main_dist_list.append(dist_list)\n",
    "main_dist_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculating the minimum distances from supermarket \n",
    "# to the property \n",
    "\n",
    "new = [] \n",
    "\n",
    "# List contains the minimum distance for all the rows\n",
    "min_dist = [] \n",
    "for i in main_dist_list:\n",
    "    new.append(i.index(min(i)))\n",
    "    \n",
    "    # Rounding off the distance to 4 decimals\n",
    "    min_dist.append(round(min(i),4)) \n",
    "column_name = np.array(new) \n",
    "\n",
    "# Creating a new column which will contain the \n",
    "# corresponding indices of supermarkets\n",
    "\n",
    "df_final['supermarket_index'] = column_name \n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the final dataframe with the appropriate supermarket id\n",
    "# and their minimum distances from the properties\n",
    "\n",
    "# Closest supermarket\n",
    "supermarket_id = []\n",
    "for item, row in df_final.iterrows():\n",
    "    \n",
    "    supermarket_id.append(df_supermarket.loc[row['supermarket_index'], 'id'])\n",
    "\n",
    "df_final['Supermarket_id'] = supermarket_id\n",
    "df_final['Distance_to_supermaket'] = min_dist\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(['supermarket_index'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active train service on all weekdays\n",
    "Check which service id has trains on all the weekdays (Monday-Friday)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calender\n",
    "df_gtfs_calendar = pd.read_csv('calendar.txt', sep=\",\", header=None)\n",
    "df_gtfs_calendar.columns = [\"service_id\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"start_date\",\"end_date\"]\n",
    "df_gtfs_calendar = df_gtfs_calendar.iloc[1:]\n",
    "df_gtfs_calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service id T0 has active trains from Monday-Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travel Minutes to CBD\n",
    "Calculating the average time taken to reach the Flinders Station from each of the station that is nearest to the property.\n",
    "\n",
    "* Define the time range in which the trains must be monitored \n",
    "* For each of the station id nearest to each of the property check:\n",
    "    - for each station id, filter out all the trip ids that stop in that particular station\n",
    "    - filter out the dataframe to see the departure times between 7am and 9am\n",
    "    - on this dataframe, check if all the trips belong to 'T0' service\n",
    "    - check if each of the trip ids go to Flinders station\n",
    "        - If they do: take the difference between the times when the trip_id arrived at Flinders station and at what time did the trip_id depart from the nearest station\n",
    "        - If there is any element in the list, then take the average of that list and make a note of it\n",
    "        - Make a note of the train station for which the average is being calculated for. \n",
    "           * Also, check which all stations are just one stop_sequence before the Flinders station (For Transfer_flag column)\n",
    "        - If not: continue with the loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Convert the departure time column to time format\n",
    "df_gtfs_stop_times['departure_time'] = pd.to_datetime(df_gtfs_stop_times['departure_time'],format= '%H:%M:%S').dt.time\n",
    "\n",
    "# Range of time in which the logic has to be run (7am to 9am)\n",
    "start_date = datetime.datetime.strptime('07:00:00', '%H:%M:%S').time()\n",
    "end_date = datetime.datetime.strptime('09:00:00', '%H:%M:%S').time()\n",
    "\n",
    "# Extending the range of arrival time to 12pm\n",
    "end_date_1 = datetime.datetime.strptime('11:30:00', '%H:%M:%S').time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# empty lists to hold all the values as we go along\n",
    "main_time = []\n",
    "main_station = []\n",
    "no_transfer_list = []\n",
    "\n",
    "# Loop through the unique station ids from the final dataframe\n",
    "for i in np.unique(df_final['Train_station_id'].values):\n",
    "    travel_time = [] # List that holds all the travel time from the nearest train station\n",
    "    \n",
    "    # A temp dataframe that holds the filtered data for the station id\n",
    "    temp = df_gtfs_stop_times[df_gtfs_stop_times['stop_id'] == i]\n",
    "    \n",
    "    # Limiting the departure time to range between 7am and 9am\n",
    "    between_7_9_1 = temp.loc[(temp['departure_time'] >= start_date) & (temp['departure_time'] <= end_date)]\n",
    "    \n",
    "    # Filtering out all the 'T0' services in the above dataframe\n",
    "    between_7_9_1['service'] = between_7_9_1['trip_id'].apply(lambda x: x.split('.')[1])\n",
    "    between_7_9_1 = between_7_9_1[between_7_9_1['service'] == 'T0']\n",
    "    \n",
    "    # Looping though each of the trip_ids that depart between 7am and 9am and has service as 'T0'\n",
    "    for item, row in between_7_9_1.iterrows():\n",
    "        \n",
    "        # Another temp dataframe to hold all the trip_id filtered rows\n",
    "        temp1 = df_gtfs_stop_times[df_gtfs_stop_times['trip_id'] == row['trip_id']]   \n",
    "        \n",
    "        # Limiting the arrival time to range between 7am and 11:30pm\n",
    "        arrive_7_9 = temp1.loc[(temp1['departure_time'] >= start_date) & (temp1['departure_time'] <= end_date_1)] \n",
    "        \n",
    "        # Making the string values, float.\n",
    "        # Arranging the dataframe in ascending order based on stop_sequence\n",
    "        arrive_7_9['stop_sequence'] = arrive_7_9['stop_sequence'].astype(int)\n",
    "        arrive_7_9 = arrive_7_9.sort_values(by=['stop_sequence'])\n",
    "        \n",
    "        # Check if Flinder's station id falls in the filtered dataframe\n",
    "        if '19854' in arrive_7_9['stop_id'].values:\n",
    "            \n",
    "            # Make the 'stop_id' as the index of this dataframe\n",
    "            arrive_7_9 = arrive_7_9.set_index('stop_id')\n",
    "            \n",
    "            # Convert the arrival time column from string to time format\n",
    "            arr_time = datetime.datetime.strptime(arrive_7_9.loc['19854', 'arrival_time'], '%H:%M:%S').time()\n",
    "            \n",
    "            # Take depature time value in a variable\n",
    "            dep_time = arrive_7_9.loc[i, 'departure_time']\n",
    "            \n",
    "            # Make the dates of the arrival and departure times to 'today'\n",
    "            arr_time = datetime.datetime.combine(datetime.date.today(), arr_time)\n",
    "            dep_time = datetime.datetime.combine(datetime.date.today(), dep_time)\n",
    "            \n",
    "            if arr_time > dep_time:\n",
    "                diff_times = arr_time - dep_time \n",
    "            else:\n",
    "                diff_times = dep_time - arr_time\n",
    "            \n",
    "            # Convert the time difference to minutes and update the time list\n",
    "            minutes = diff_times.total_seconds() / 60\n",
    "            travel_time.append(minutes)\n",
    "                 \n",
    "            # To check if there is a station just one stop before Flinders Station\n",
    "            if arrive_7_9.loc['19854', 'stop_sequence'] - arrive_7_9.loc[i, 'stop_sequence'] == 1:\n",
    "                no_transfer_list.append(i)\n",
    "                \n",
    "    # Check if there is any time to calculate the average on        \n",
    "    if len(travel_time) > 0:\n",
    "        \n",
    "        # Update the list with average time\n",
    "        main_time.append(round(sum(travel_time)/len(travel_time)))\n",
    "        main_station.append(i)\n",
    "        \n",
    "        \n",
    "# Refer: https://stackoverflow.com/questions/43305577/python-calculate-the-difference-between-two-datetime-time-objects/43308104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to hold all the station id and their \n",
    "# corresponding minutes to travel\n",
    "\n",
    "minutes_from_station = pd.DataFrame(columns=['station_id', 'time'])\n",
    "minutes_from_station['station_id'] = main_station\n",
    "minutes_from_station['time'] = main_time\n",
    "\n",
    "# # Reset the index of the final Dataframe\n",
    "# df_final = df_final.reset_index()\n",
    "\n",
    "# Defualt value of 'travel_min_to_CBD'\n",
    "df_final['travel_min_to_CBD'] = 0\n",
    "\n",
    "df_final_copy = df_final.copy()\n",
    "\n",
    "# loop through each row of final dataframe and update the travel\n",
    "# min to CBD\n",
    "for item, row in df_final_copy.iterrows():\n",
    "    for item1, row1 in minutes_from_station.iterrows():\n",
    "        if row['Train_station_id'] == row1['station_id']:\n",
    "            df_final.loc[item, 'travel_min_to_CBD'] = row1['time']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default value of transfer flag is -1\n",
    "df_final['Transfer_flag'] = -1\n",
    "\n",
    "# Loop through the final dataframe and assign 1 or 0 for transfer flag column \n",
    "for i, r in df_final_copy.iterrows():\n",
    "    if r['Train_station_id'] in minutes_from_station['station_id'].values:\n",
    "        df_final.loc[i, 'Transfer_flag'] = 1\n",
    "        \n",
    "    if r['Train_station_id'] == no_transfer_list[0]:\n",
    "        df_final.loc[i, 'Transfer_flag'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suburb\n",
    "* Read the shapes datafiles into a dataframe\n",
    "* Create a function to calculate the polygon coordinates and retrieve the suburb name\n",
    "    - Check if the point object lies within the polygon provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "\n",
    "shape_file = geopandas.read_file(\"VIC_LOCALITY_POLYGON_shp.shp\")\n",
    "shape_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the suburb names\n",
    "def suburb_names(lon, lat, loca_geo):\n",
    "    point = Point(lon, lat)\n",
    "    for key, value in loca_geo.items():\n",
    "        p = Polygon(value)\n",
    "        if p.contains(point):\n",
    "            return key\n",
    "    return 'not available'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "# Default value of suburb coulmn is 'not available'\n",
    "df_final['suburb'] = 'not available'\n",
    "\n",
    "# Forming a dict from shape_file \n",
    "loca_geo = dict(zip(shape_file['VIC_LOCA_2'].values, shape_file['geometry'].values))\n",
    "\n",
    "# Updating the suburb column in the final dataframe\n",
    "df_final['suburb'] = df_final.apply(lambda geo: suburb_names(geo['lng'], geo['lat'], loca_geo), axis= 1)\n",
    "\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming property_id with Property_id\n",
    "df_final.columns=[it.replace('property_id','Property_id') for it in df_final.columns]\n",
    "\n",
    "\n",
    "# Reordering the dataframe\n",
    "df_final = df_final[['Property_id', 'lat', 'lng', 'addr_street', 'suburb', 'price', 'property_type', 'year', 'bedrooms', 'bathrooms', 'parking_space', 'Shopping_center_id', 'Distance_to_sc', 'Train_station_id', 'Distance_to_train_station', 'travel_min_to_CBD', 'Transfer_flag', 'Hospital_id', 'Distance_to_hospital','Supermarket_id', 'Distance_to_supermaket' ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it to csv\n",
    "df_final.to_csv('31009751_A3_solution.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "A final csv is written with 21 columns as asked in the specifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the remaining files which are not used in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes\n",
    "df_gtfs_shapes = pd.read_csv('shapes.txt', sep=\",\", header=None)\n",
    "df_gtfs_shapes.columns = [\"shape_id\", \"shape_pt_lat\", \"shape_pt_lon\", \"shape_pt_sequence\",\"shape_dist_traveled\"]\n",
    "df_gtfs_shapes = df_gtfs_shapes.iloc[1:]\n",
    "df_gtfs_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routes\n",
    "df_gtfs_routes = pd.read_csv('routes.txt', sep=\",\", header=None)\n",
    "df_gtfs_routes.columns = [\"route_id\",\"agency_id\",\"route_short_name\",\"route_long_name\",\"route_type\"]\n",
    "df_gtfs_routes = df_gtfs_routes.iloc[1:]\n",
    "df_gtfs_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calender dates\n",
    "df_gtfs_calendar_dates = pd.read_csv('calendar_dates.txt', sep=\",\", header=None)\n",
    "df_gtfs_calendar_dates.columns = [\"service_id\",\"date\",\"exception_type\"]\n",
    "df_gtfs_calendar_dates = df_gtfs_calendar_dates.iloc[1:]\n",
    "df_gtfs_calendar_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calender\n",
    "df_gtfs_calendar = pd.read_csv('calendar.txt', sep=\",\", header=None)\n",
    "df_gtfs_calendar.columns = [\"service_id\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"start_date\",\"end_date\"]\n",
    "df_gtfs_calendar = df_gtfs_calendar.iloc[1:]\n",
    "df_gtfs_calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agency\n",
    "df_gtfs_agency = pd.read_csv('agency.txt', sep=\",\", header=None)\n",
    "df_gtfs_agency.columns = [\"agency_id\",\"agency_name\",\"agency_url\",\"agency_timezone\",\"agency_lang\"]\n",
    "df_gtfs_agency = df_gtfs_agency.iloc[1:]\n",
    "df_gtfs_agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Data Reshaping\n",
    "Observing the effect of different normalization/transformation methods on the “price” ,\n",
    "“Distance_to_sc”, “travel_min_to_CBD” , and “Distance_to_hospital” attributes. Following are the different methods used:\n",
    "\n",
    "1. Standardization\n",
    "2. Minmax normalization \n",
    "3. log transformation \n",
    "4. power transformation \n",
    "5. box-cox transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation\n",
    "Standardized values (also called standard scores or normal deviates) are the same thing as z-scores. So, it follows the formula: \n",
    "\n",
    "$$z = \\frac{x - mean(x)}{\\sigma}$$\n",
    "\n",
    "This makes sure that each column that we are interested in has values around the value '0'.\n",
    "\n",
    "Also, this is used to make all the column's unit values standard. For eg., we want to predict the 'price' (unit: Dollars) having distance to shopping center (unit: Kilometer), distance to hospital (unit: Kilometer) and travel minute to CBD (unit: minutes). To bring all the columns to a standard form, irrespective of the units that each of them hold, standardization is carried out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['price', 'Dist_sc', 'travel', 'Dist_hospital'])\n",
    "df['price'] = df_final['price'].values.astype(float)\n",
    "df['Dist_sc'] = df_final['Distance_to_sc'].values.astype(float)\n",
    "df['travel'] = df_final['travel_min_to_CBD'].values.astype(float)\n",
    "df['Dist_hospital'] = df_final['Distance_to_hospital'].values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before standardizing the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dist_hospital'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['travel'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "names = df.columns\n",
    "\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After standardizing the columns\n",
    "* Observe the x axis values after standardizing the column. The values are in and around '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df['price'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df['Dist_sc'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df['Dist_hospital'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df['travel'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the price for standardized columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression for standardized dataset\n",
    "dependent = scaled_df['price']\n",
    "independent = scaled_df[['Dist_sc', 'travel','Dist_hospital']]\n",
    "\n",
    "# Using Linear Regression Model from Sklearn \n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(independent, dependent)\n",
    "\n",
    "std_predict = linear_regression.predict(independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIN/MAX\n",
    "\n",
    "Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler object\n",
    "minMax_transform = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Transforming all the required columns\n",
    "price = minMax_transform.fit_transform(df[['price']])\n",
    "Dist_sc = minMax_transform.fit_transform(df[['Dist_sc']])\n",
    "travel = minMax_transform.fit_transform(df[['travel']])\n",
    "Dist_hospital = minMax_transform.fit_transform(df[['Dist_hospital']])\n",
    "\n",
    "# Updating the dataframe with transformed values\n",
    "df_final['price_minmax'] = pd.DataFrame(price)\n",
    "df_final['Distance_to_sc_minmax'] = pd.DataFrame(Dist_sc)\n",
    "df_final['travel_min_to_CBD_minmax'] = pd.DataFrame(travel)\n",
    "df_final['Distance_to_hospital_minmax'] = pd.DataFrame(Dist_hospital)\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Min/Max Normalization\n",
    "\n",
    "All the values are scaled between 0 and 1. 0 being the minimum in of that column, 1 being maximum of that column, 0.5 being halfway through in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['price_minmax'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Distance_to_sc_minmax'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['travel_min_to_CBD_minmax'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Distance_to_hospital_minmax'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression for standardized dataset\n",
    "dependent = df_final['price_minmax']\n",
    "independent = df_final[['Distance_to_sc_minmax', 'travel_min_to_CBD_minmax','Distance_to_hospital_minmax']]\n",
    "\n",
    "# Using Linear Regression Model from Sklearn \n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(independent, dependent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_predict = linear_regression.predict(independent)\n",
    "minmax_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log transformation\n",
    "* Taking log of each column and then predicting the values\n",
    "* Since there are 0 values in travel_min_to_CBD column, taking log of 0 will result in '-inf' . Hence impute '0' with '0.001' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate natural logarithm on the log transformation \n",
    "df_final['price'] = df_final[['price']].values.astype(float)\n",
    "df_final['Distance_to_sc'] = df_final[['Distance_to_sc']].values.astype(float)\n",
    "df_final['travel_min_to_CBD'] = df_final[['travel_min_to_CBD']].values.astype(float)\n",
    "df_final['Distance_to_hospital'] = df_final[['Distance_to_hospital']].values.astype(float)\n",
    "\n",
    "# Imputing 0's with 0.001 and then taking the log of the column\n",
    "df_final['travel_min_to_CBD_log'] = df_final['travel_min_to_CBD'].values\n",
    "\n",
    "df_final['travel_min_to_CBD_log'] = df_final['travel_min_to_CBD_log'].apply(lambda x: 0.001 if x == 0 else x)\n",
    "\n",
    "df_final[['price_log']] = np.log(df_final['price'])\n",
    "df_final[['Distance_to_sc_log']] = np.log(df_final['Distance_to_sc'])\n",
    "df_final[['travel_min_to_CBD_log']] = np.log(df_final['travel_min_to_CBD_log'])\n",
    "df_final[['Distance_to_hospital_log']] = np.log(df_final['Distance_to_hospital'])\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms for each of the log transformed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['price_log'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Distance_to_sc_log'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['travel_min_to_CBD_log'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Distance_to_hospital_log'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression for standardized dataset\n",
    "dependent = df_final['price_log']\n",
    "independent = df_final[['Distance_to_sc_log', 'travel_min_to_CBD_log','Distance_to_hospital_log']]\n",
    "\n",
    "# Using Linear Regression Model from Sklearn \n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(independent, dependent)\n",
    "\n",
    "log_predict = linear_regression.predict(independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['price_log']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power transformation (box-cox)\n",
    "A Box Cox transformation is a transformation of a non-normal dependent variables into a normal shape. Here, since none of the values should be 0 or lesser than 0, I am imputing 0.001 inplace of 0.\n",
    "\n",
    "Refer: https://www.statisticshowto.com/box-cox-transformation/#:~:text=What%20is%20a%20Box%20Cox,a%20broader%20number%20of%20tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['price', 'Dist_sc', 'travel', 'Dist_hospital'])\n",
    "df['price'] = df_final['price']\n",
    "df['Dist_sc'] = df_final['Distance_to_sc']\n",
    "df['travel'] = df_final['travel_min_to_CBD']\n",
    "df['Dist_hospital'] = df_final['Distance_to_hospital']\n",
    "\n",
    "df['travel'] = df['travel'].apply(lambda x: 0.001 if x == 0 else x)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "power_transformer = PowerTransformer(method = 'box-cox')\n",
    "power_transformer.fit(df)                      \n",
    "train_power_transformer = power_transformer.transform(df)\n",
    "transform_df = pd.DataFrame(train_power_transformer, columns = ['price', 'Dist_sc', 'travel', 'Dist_hospital'])\n",
    "transform_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a power transform that assumes the values of the input variable to which it is applied are strictly positive. That means 0 and negative values are not supported. It is important to note that the Box-Cox procedure can only be applied to data that is strictly positive.\n",
    "\n",
    "Refer: https://machinelearningmastery.com/power-transforms-with-scikit-learn/#:~:text=Box%2DCox%20Transform,-The%20Box%2DCox&text=It%20is%20a%20power%20transform,data%20that%20is%20strictly%20positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression for standardized dataset\n",
    "dependent = transform_df['price']\n",
    "independent = transform_df[['Dist_sc', 'travel','Dist_hospital']]\n",
    "\n",
    "# Using Linear Regression Model from Sklearn \n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(independent, dependent)\n",
    "\n",
    "bcox_predict = linear_regression.predict(independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_df['price'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_df['Dist_sc'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_df['travel'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_df['Dist_hospital'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Standardized rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for standardized \n",
    "mean_squared_error(scaled_df['price'].values, std_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Min/Max rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for min/max rmse\n",
    "mean_squared_error(df_final['price_minmax'].values, minmax_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for Log transformation\n",
    "\n",
    "mean_squared_error(df_final['price_log'].values, log_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Box-cox Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE for Box-cox transformation\n",
    "\n",
    "mean_squared_error(transform_df['price'].values, bcox_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Transformations in general, are applied to make the data more testable on different models. And inorder to evaluate which model is better than which, we need to know the RMSE scores for each models and narrow down to the one which has the least RMSE score.\n",
    "\n",
    "From our observation, for this dataset, Min/Max transformation will give a better prediction than the rest of them since it has the least rmse score of 0.01617. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
